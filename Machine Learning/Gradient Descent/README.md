# Gradient Descent

https://piazza.com/class_profile/get_resource/k1zer6tg6s04bb/k68sgzg7o0646v

Batch gradient descent 

​	◦ Utilize the gradient of all the data 

​	◦ Slow: need to consider all the data before making a single update





![image.png](https://i.loli.net/2020/02/11/cY1m3xL45gfjIA9.png)



















### Statquest

> MLE/ Backprop
>
> Gradient Descent
>
> - Step size = partial derivative * learning rate
> - Learning rate can be adjusted in practice (Adam)
> - Stochastic gradient descent (randomly draw one sample) can be used to solve computer power problem



![image.png](https://i.loli.net/2020/01/08/LMyJxtAmGh2P1VT.png)



![image.png](https://i.loli.net/2020/01/08/zsPbTFahqgvIGf6.png)



It's like what we did in **backpropagation**, set initial guess and update the coefficients based on learning rate and derivatives. 



[Stochastic Gradient Descent, Clearly Explained!!!](https://www.youtube.com/watch?v=vMh0zPT0tLI&feature=youtu.be)



![Stochastic Gradient Descent.gif](https://i.loli.net/2020/01/08/YEjAcgrywVs2p8u.gif)





