# Gradient Descent

> MLE/ Backprop
>
> Gradient Descent
>
> - Step size = partial derivative * learning rate
> - Learning rate can be adjusted in practice (Adam)
> - Stochastic gradient descent (randomly draw one sample) can be used to solve computer power problem



![image.png](https://i.loli.net/2020/01/08/LMyJxtAmGh2P1VT.png)



![image.png](https://i.loli.net/2020/01/08/zsPbTFahqgvIGf6.png)



It's like what we did in **backpropagation**, set initial guess and update the coefficients based on learning rate and derivatives. 



[Stochastic Gradient Descent, Clearly Explained!!!](https://www.youtube.com/watch?v=vMh0zPT0tLI&feature=youtu.be)



![Stochastic Gradient Descent.gif](https://i.loli.net/2020/01/08/YEjAcgrywVs2p8u.gif)





