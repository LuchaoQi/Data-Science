[toc]







### [Assumptions of logistic regression](https://www.statology.org/assumptions-of-logistic-regression/)

First, logistic regression does not require a linear relationship between the dependent and independent variables. Second, the error terms (residuals) do not need to be normally distributed. Third, homoscedasticity is not required. Finally, the dependent variable in logistic regression is not measured on an interval or ratio scale.

So the assumptions of logistic regression are:

1. iid and binary observations
2. iid variables
3. linear relationship between variables and logit of response
4. large sample size



### Logistic regression



[Why is the log likelihood of logistic regression concave?](https://homes.cs.washington.edu/~marcotcr/blog/concavity/)

[What is the difference between convex and non-convex optimization problems?](https://www.researchgate.net/post/What_is_the_difference_between_convex_and_non-convex_optimization_problems)

https://seeing-theory.brown.edu/bayesian-inference/

check the derivative of likelihood function which helps visually and intuitively understanding why it's concave

![image.png](https://i.loli.net/2020/01/14/RpJxmaFH7y4NX1g.png)



https://piazza.com/class_profile/get_resource/k1zer6tg6s04bb/k68sgzg7o0646v

For MLE

Problem 

​	◦ ==No closed form (analytical) solution for w==

That's why we usually do gradient descent in optimization





























[Notes from Coursera Deep Learning courses by Andrew Ng](https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng?from_action=save)

![](https://i.loli.net/2020/01/03/wkKFegULahqbJGM.png)











Loss function could be understood from MLE or gradient descent (Cross Entropy)

[Demystifying Cross-Entropy](https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8)

[Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)

![image.png](https://i.loli.net/2020/01/07/S8cEajbtuhZ59KQ.png)

[Loss Function (Part II): Logistic Regression](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11)









