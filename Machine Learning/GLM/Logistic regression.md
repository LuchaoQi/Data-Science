### Logistic regression



[Why is the log likelihood of logistic regression concave?](https://homes.cs.washington.edu/~marcotcr/blog/concavity/)

[What is the difference between convex and non-convex optimization problems?](https://www.researchgate.net/post/What_is_the_difference_between_convex_and_non-convex_optimization_problems)

https://seeing-theory.brown.edu/bayesian-inference/

check the derivative of likelihood function which helps visually and intuitively understanding why it's concave

![image.png](https://i.loli.net/2020/01/14/RpJxmaFH7y4NX1g.png)



https://piazza.com/class_profile/get_resource/k1zer6tg6s04bb/k68sgzg7o0646v

For MLE

Problem 

​	◦ ==No closed form (analytical) solution for w==

That's why we usually do gradient descent in optimization





























[Notes from Coursera Deep Learning courses by Andrew Ng](https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng?from_action=save)

![](https://i.loli.net/2020/01/03/wkKFegULahqbJGM.png)











Loss function could be understood from MLE or gradient descent (Cross Entropy)

[Demystifying Cross-Entropy](https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8)

[Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)

![image.png](https://i.loli.net/2020/01/07/S8cEajbtuhZ59KQ.png)

[Loss Function (Part II): Logistic Regression](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11)









