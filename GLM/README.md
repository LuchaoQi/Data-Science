### Logistic regression

Loss function could be understood from MLE or gradient descent (Cross Entropy)

[Demystifying Cross-Entropy](https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8)

[Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)

![image.png](https://i.loli.net/2020/01/07/S8cEajbtuhZ59KQ.png)

[Loss Function (Part II): Logistic Regression](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11)

![](https://i.loli.net/2020/01/03/wkKFegULahqbJGM.png)

### Notes

[假设检验](https://zhuanlan.zhihu.com/p/32750128)

[Why we use error term?](https://www.quora.com/Why-do-we-need-an-error-term-in-regression-model-What-is-its-statistical-distribution)

**Always check non-linearity/co-linearity - VIF/avPlots**

[Detecting Multicollinearity](https://www.edupristine.com/blog/detecting-multicollinearity)

[Partial regression plots](https://rpubs.com/Hank_Stevens/prp)

[What does an Added Variable Plot (Partial Regression Plot) explain in a multiple regression?](https://stats.stackexchange.com/questions/125561/what-does-an-added-variable-plot-partial-regression-plot-explain-in-a-multiple)



