# Sequence to Sequence Model

[Attention â€” Seq2Seq Models](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)

http://web.stanford.edu/class/cs20si/lectures/slides_13.pdf

[Intuitive Understanding of Seq2seq model & Attention Mechanism in Deep Learning](https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e)

[Sequence-to-sequence Models](https://nlp.stanford.edu/~johnhew/public/14-seq2seq.pdf)



tf-seq2seq is a general-purpose encoder-decoder framework for Tensorflow that can be used for **Machine Translation, Text Summarization, Conversational Modeling, Image Captioning**, and more.

![](https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif)





![](https://miro.medium.com/max/5120/1*LYGO4IxqUYftFdAccg5fVQ.png)





### Code example

[Difference Between Return Sequences and Return States for LSTMs in Keras](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/)



[seq2seq (Sequence to Sequence) Model for Deep Learning with PyTorch](https://www.guru99.com/seq2seq-model.html)



[A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)



[How to implement Seq2Seq LSTM Model in Keras #ShortcutNLP](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)



