RNN to seq2seq

### RNN cheat sheet

https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks



![image.png](https://i.loli.net/2020/01/10/jwzfiMNeKsnL2Q3.png)



The input sequence x=(x⟨1⟩,x⟨2⟩,...,x⟨Tx⟩)x=(x⟨1⟩,x⟨2⟩,...,x⟨Tx⟩) is carried over $T_x$ time steps. The network outputs y=(y⟨1⟩,y⟨2⟩,...,y⟨Tx⟩)y=(y⟨1⟩,y⟨2⟩,...,y⟨Tx⟩).

In NLP, feed in words like `n/a/m/a/s/k/a/r` subsequently at each time step.





![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn.png)

$x^{<t>}$ is the input at time t



### Examples

[RNN(Recurrent Neural Network) Tutorial: TensorFlow Example](https://www.guru99.com/rnn-tutorial.html)



[Building your Recurrent Neural Network - Step by Step](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html)



[namaskar example](https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860)



![](https://hackernoon.com/hn-images/1*_mM83sFLjzKt8cRB439Y3Q.gif)











