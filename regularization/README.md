# Regularization

## Resources

[Intuitions on L1 and L2 Regularisation](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261)

https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net

https://drsimonj.svbtle.com/ridge-regression-with-glmnet



[Regularization Part 1: L2, Ridge Regression](https://youtu.be/Q81RR3yKn30)

[Regularization Part 2: L1, Lasso Regression](https://youtu.be/NGf0voTMlcs)

[Regularization Part 3: Elastic-Net Regression](https://youtu.be/1dKRdX9bfIo)

[Regularization Part 4: Ridge, Lasso and Elastic-Net Regression in R](https://youtu.be/ctmNq7FgbvI)

pretty detailed explanation

[Data Science - Part XII - Ridge Regression, LASSO, and Elastic Nets](https://www.youtube.com/watch?v=ipb2MhSRGdw)



## Intuitions

[Data Science - Part XII - Ridge Regression, LASSO, and Elastic Nets](https://www.youtube.com/watch?v=ipb2MhSRGdw)

![image-20191212161333216](https://github.com/LuchaoQi/Machine-Learning/blob/master/regularization/image-20191212161333216.png?raw=true)

> Understanding **constrained (penalized) form**
>
> https://stats.stackexchange.com/a/349056 
>
> [Why are additional constraint and penalty term equivalent in ridge regression?](https://math.stackexchange.com/questions/335306/why-are-additional-constraint-and-penalty-term-equivalent-in-ridge-regression)
>
> Note, regularizer's power will help you remember terminology
>
> So the following image makes more sense

![image-20191212162141455](https://github.com/LuchaoQi/Machine-Learning/blob/master/regularization/image-20191212162141455.png?raw=true)

> [Regularization Part 2: Lasso Regression](https://www.youtube.com/watch?v=NGf0voTMlcs&feature=youtu.be)
>
> That's why some people use Lasso to "eliminate" lots of useless variables (i.e. $\beta = 0$) .

![image-20191212131239239](https://github.com/LuchaoQi/Machine-Learning/blob/master/regularization/image-20191212131239239.png?raw=true)

