# Gradient Descent

> MLE/ Backprop



![image.png](https://i.loli.net/2020/01/08/LMyJxtAmGh2P1VT.png)



![image.png](https://i.loli.net/2020/01/08/zsPbTFahqgvIGf6.png)



It's like what we did in **backpropagation**, set initial guess and update the coefficients based on learning rate and derivatives. 



[Stochastic Gradient Descent, Clearly Explained!!!](https://www.youtube.com/watch?v=vMh0zPT0tLI&feature=youtu.be)



![Stochastic Gradient Descent.gif](https://i.loli.net/2020/01/08/YEjAcgrywVs2p8u.gif)





