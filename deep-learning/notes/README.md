Back propagation

> Get weights through calculus and average over all desired changes of weights
>
> https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4
>
> Mini-batches is able to trade-off the convergence speed and computation efficiency

---

Loss/cost function

> Add up the squares of differences between trash output from network and the value you want.
>
> Get local minimum of cost function: when the slope is flattening out towards the minimum your steps get smaller to help you from `overshooting` . 
>
> For multivariable calculus, check chain rule. Imagine a ball rolling down a hill. All we need to do is to get the minima so calculate gradient and return it back to parameters.

------

Know what's Neural Network

> https://medium.com/@srnghn/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4
>
> Next:
>
> Know how to auto learn/adjust network
>
> Optimization of network

---

> Code
>
> https://github.com/LuchaoQi/deep-learning
>
> Project
>
> https://www.d2l.ai/chapter_optimization/minibatch-sgd.html
>
> http://neuralnetworksanddeeplearning.com/
>
> Video
>
> https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi



