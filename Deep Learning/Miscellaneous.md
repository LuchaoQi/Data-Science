### Miscellaneous

Background

https://github.com/bcaffo/ds4bme 

http://pzs.dstu.dp.ua/DataMining/bibl/practical/R%20Deep%20Learning%20Essentials.pdf

http://deeplearning.mit.edu](http://deeplearning.mit.edu/

Intro

https://medium.com/@srnghn/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4

https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi

Know what's Neural Network

https://medium.com/@srnghn/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4

Project

https://www.d2l.ai/chapter_optimization/minibatch-sgd.html

http://neuralnetworksanddeeplearning.com/

Video

https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi 

Loss/cost function

Add up the squares of differences between trash output from network and the value you want.

Get local minimum of cost function: when the slope is flattening out towards the minimum your steps get smaller to help you from `overshooting` . 

For multivariable calculus, check chain rule. Imagine a ball rolling down a hill. All we need to do is to get the minima so calculate gradient and return it back to parameters.

Back propagation

Get weights through calculus and average over all desired changes of weights

https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4

Mini-batches is able to trade-off the convergence speed and computation efficiency