

[toc]



### https://cs224d.stanford.edu/lecture_notes/notes4.pdf







![](https://i.loli.net/2020/05/28/mDG1Zpqd4z2rtRU.png)







### RNN cheat sheet

https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks



![](https://i.loli.net/2020/01/10/jwzfiMNeKsnL2Q3.png)



The input sequence x=(x⟨1⟩,x⟨2⟩,...,x⟨Tx⟩)x=(x⟨1⟩,x⟨2⟩,...,x⟨Tx⟩) is carried over $T_x$ time steps. The network outputs y=(y⟨1⟩,y⟨2⟩,...,y⟨Tx⟩)y=(y⟨1⟩,y⟨2⟩,...,y⟨Tx⟩).

In NLP, feed in words like `n/a/m/a/s/k/a/r` subsequently at each time step.

$x^{<t>}$ is the input at time t





![](https://i.loli.net/2020/05/23/Vo4SYDRjGa1rEPJ.png)



### Code Examples

[RNN(Recurrent Neural Network) Tutorial: TensorFlow Example](https://www.guru99.com/rnn-tutorial.html)



[Building your Recurrent Neural Network - Step by Step](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html)



==[namaskar example](https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860)==



![](https://hackernoon.com/hn-images/1*_mM83sFLjzKt8cRB439Y3Q.gif)











