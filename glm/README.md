### Logistic regression

Loss function could be understood from MLE or gradient descent (Cross Entropy)

[Demystifying Cross-Entropy](https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8)

![](https://miro.medium.com/max/1096/1*rdBw0E-My8Gu3f_BOB6GMA.png)

https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11

![](https://i.loli.net/2020/01/03/wkKFegULahqbJGM.png)

### Notes

[假设检验](https://zhuanlan.zhihu.com/p/32750128)

[Why we use error term?](https://www.quora.com/Why-do-we-need-an-error-term-in-regression-model-What-is-its-statistical-distribution)

**Always check non-linearity/co-linearity - VIF/avPlots**

[Detecting Multicollinearity](https://www.edupristine.com/blog/detecting-multicollinearity)

[Partial regression plots](https://rpubs.com/Hank_Stevens/prp)

[What does an Added Variable Plot (Partial Regression Plot) explain in a multiple regression?](https://stats.stackexchange.com/questions/125561/what-does-an-added-variable-plot-partial-regression-plot-explain-in-a-multiple)



