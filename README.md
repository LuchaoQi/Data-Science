<<<<<<< HEAD
# Machine Learning

> Currently, there are thousands of resources online to introduce Machine Learning. This repo contains some awesome resources I personally suggest and summaries of different models for review purpose.
>
> I would suggest looking at [statquest](https://statquest.org/video-index/) at first to have general understandings of some basic algorithms.
>
> **STATQUEST!!!**

## Miscellaneous

some real python code to achieve different dimension reduction algorithms

https://github.com/heucoder/dimensionality_reduction_alo_codes

some interesting ML projects with tensorflow code

[Google机器学习速成课程Code](https://github.com/yuanxiaosc/Google-Machine-learning-crash-course/tree/master/Google%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%9F%E6%88%90%E8%AF%BE%E7%A8%8BCode)

[Kaggle竞赛宝典方案汇总！](https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247487863&idx=1&sn=bd479caf0feaca452e65b0da50c32d33&chksm=e870c03adf07492c21ac3e9072eda4e9446dd473b421583f00b9e638afafcb87237a12846a40&mpshare=1&scene=1&srcid=&sharer_sharetime=1576339122943&sharer_shareid=54d7b6bf73b347d381a7bff3f78b99d1&key=b572ae670c77797d7923c11b5149181658a309ad3b8b574fa2003ffa151ac06f8426677e83ed063be010dd60dbc73b4d6bbd772d8b50f9b17a82214626f5192680b3d8a672709b3342342a67d533d936&ascene=1&uin=NzA3NTE3MTMz&devicetype=Windows+10&version=62070158&lang=en&exportkey=A0kdpkaj6IY9dwGnqyI1808%3D&pass_ticket=mXiZPxVUTeF0VnnJb4AHwEvTWSaICckEJohlaC3yc%2FP8iL5qOjrYRgxs%2FybtxOwq)

[本科生晋升GM记录 & Kaggle比赛进阶技巧分享](https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247487590&idx=1&sn=ae9ebf3f718690240259daed54a4084e&chksm=e870c12bdf07483dc053f656ac01612db7c441b821cdff984c66b15bde23bd9a9305de14c30e&mpshare=1&scene=1&srcid=&sharer_sharetime=1575125847729&sharer_shareid=54d7b6bf73b347d381a7bff3f78b99d1&key=de32ce037b85eff0c7121abc155c64f041fff038718ea39c0d24512ad7703e1ffa07c5d8b2d7237faa907d0d22ffbb903a933c027fead6db0b73c6bde6704aaad223e99d54330ce04b3b3f55b089b7a0&ascene=1&uin=NzA3NTE3MTMz&devicetype=Windows+10&version=62070158&lang=en&pass_ticket=%2BmIfHViH%2B%2FpCIVsCC2vbH78SdAZ%2BOqB2BifvUTZaQ4xrhrTxgR1IpaFkFbylBEwk)













=======
# deep-learning
Background

> https://github.com/bcaffo/ds4bme 
>
> http://pzs.dstu.dp.ua/DataMining/bibl/practical/R%20Deep%20Learning%20Essentials.pdf
>
> http://deeplearning.mit.edu](http://deeplearning.mit.edu/

Intro

> https://medium.com/@srnghn/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4
>
> https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi

Know what's Neural Network

> https://medium.com/@srnghn/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4
>
> Project
>
> https://www.d2l.ai/chapter_optimization/minibatch-sgd.html
>
> http://neuralnetworksanddeeplearning.com/
>
> Video
>
> https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi 

Loss/cost function

> Add up the squares of differences between trash output from network and the value you want.
>
> Get local minimum of cost function: when the slope is flattening out towards the minimum your steps get smaller to help you from `overshooting` . 
>
> For multivariable calculus, check chain rule. Imagine a ball rolling down a hill. All we need to do is to get the minima so calculate gradient and return it back to parameters.

Back propagation

> Get weights through calculus and average over all desired changes of weights
>
> https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4
>
> Mini-batches is able to trade-off the convergence speed and computation efficiency
>>>>>>> remotes/OldRepo/master
